from datasets import load_dataset
import json
import os
from coding_agent import main as run_greycells
from coding_agent import execute_code # å¼•å…¥ execute_code ç”¨äºå¤–éƒ¨éªŒè¯
all_stats = []
def verify_with_canonical_test(generated_code, problem):
    """
    ä½¿ç”¨ HumanEval å®˜æ–¹æä¾›çš„æµ‹è¯•ç”¨ä¾‹è¿›è¡Œå¤–éƒ¨éªŒè¯
    """
    # HumanEval çš„ test å­—æ®µåŒ…å« check å‡½æ•°ï¼Œæˆ‘ä»¬éœ€è¦æŠŠç”Ÿæˆçš„ä»£ç æ‹¼ä¸Šå»
    # å¹¶æ˜¾å¼è°ƒç”¨ check(entry_point)
    verification_content = f"""
{generated_code}

{problem['test']}

check({problem['entry_point']})
"""
    # æ„é€ æ²™ç®±æ‰§è¡ŒåŒ…
    source_dict = {
        "content": verification_content,
        "filename": "final_verify.py",
        "packages": [] 
    }
    dummy_test = {"filename": "dummy.py", "content": "import unittest\nclass D(unittest.TestCase): pass"}
    
    # é™é»˜æ‰§è¡Œ
    result = execute_code(source_dict, dummy_test, run_command="python final_verify.py")
    print(result)
    return result.get("is_pass", False), result.get("stderr", "")

def run_humaneval_benchmark(dataset, limit=5):
    results = []
    is_break = False
    # [FIX 1] ä½¿ç”¨ range éå†ï¼Œé¿å…å­—å…¸åˆ‡ç‰‡é™·é˜±
    for i in range(min(limit, len(dataset))):
        problem = dataset[i]
        task_id = problem["task_id"]
        user_prompt = problem["prompt"]
        
        print(f"\n" + "="*50)
        print(f"ğŸ“‹ å¤„ç†ä»»åŠ¡: {task_id}")
        print("="*50)

        try:
            # [FIX 2] åˆ é™¤ä¸å­˜åœ¨çš„ return_code å‚æ•°
            outcome = run_greycells(prompt=user_prompt, max_loop_count=3, return_code=True)
            
            stats = {"task_id": task_id,"tokens": outcome["tokens"], "loops": outcome["loops"], "calls": outcome["calls"]}
            all_stats.append(stats)
            internal_pass = outcome.get("is_pass", False)
            generated_code = outcome.get("code", {}).get("content", "")

            # [FIX 3] å¢åŠ å¤–éƒ¨éªŒè¯ (Ground Truth)
            external_pass = False
            error_msg = ""
            
            if internal_pass:
                print(f"   â†³ å†…éƒ¨æµ‹è¯•é€šè¿‡ï¼Œæ­£åœ¨è¿›è¡Œå®˜æ–¹éªŒè¯...")
            else:
                print(f"   â†³ å†…éƒ¨æµ‹è¯•æœªé€šè¿‡...")
                break

            external_pass, error_msg = verify_with_canonical_test(generated_code, problem)
            
            status = "âŒ FAIL"
            if internal_pass and external_pass:
                status = "âœ… PASS"
            elif internal_pass and not external_pass:
                status = "âš ï¸ FALSE POSITIVE" # å†…éƒ¨è¿‡äº†ï¼Œå¤–éƒ¨æ²¡è¿‡ï¼ˆå¹»è§‰ï¼‰
            
            print(f"   â†³ ç»“æœ: {status}")

            results.append({
                "task_id": task_id,
                "internal_pass": internal_pass,
                "external_pass": external_pass
            })
                    
        except Exception as e:
            print(f"ğŸ’¥ å¼‚å¸¸: {e}")
            results.append({"task_id": task_id, "internal_pass": False, "external_pass": False, "error": str(e)})
            # with open("logs")

    # æ‰“å°æœ€ç»ˆæŠ¥å‘Š
    print("\n" + "ğŸ“Š" * 5 + " BENCHMARK REPORT " + "ğŸ“Š" * 5)
    total = len(results)
    internal_ok = sum(1 for r in results if r["internal_pass"])
    real_pass = sum(1 for r in results if r["external_pass"])
    
    print(f"æ€»ä»»åŠ¡æ•°: {total}")
    print(f"å†…éƒ¨é€šè¿‡ç‡ (Self-Reported): {internal_ok}/{total}")
    print(f"çœŸå®é€šè¿‡ç‡ (HumanEval Score): {real_pass}/{total}  <-- è¿™æ˜¯ä½ çš„è®ºæ–‡åˆ†æ•°")

if __name__ == "__main__":
    # åŠ è½½æ•°æ®é›†
    print("ğŸ“¥ Loading HumanEval...")
    dataset = load_dataset("openai/openai_humaneval")
    # ä¼ å…¥ test split
    run_humaneval_benchmark(dataset['test'], limit=10)
    print(all_stats)