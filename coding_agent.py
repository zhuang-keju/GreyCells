
import os
import sys
import json
import re
import subprocess
import time
import argparse
import urllib.request
import urllib.error
from typing import Dict, Any, Tuple, Optional



from e2b_code_interpreter import Sandbox
from dotenv import load_dotenv



# ============================================================================
# Configuration & Constants
# ============================================================================

load_dotenv()

DEFAULT_MODEL = "gemini-2.0-flash"  # Default if env var not set
API_KEY_ENV = "LLM_API_KEY"
PROVIDER_ENV = "LLM_PROVIDER"
MODEL_ENV = "LLM_MODEL"
E2B_API_KEY = os.environ.get("E2B_API_KEY")

if not E2B_API_KEY:
    print("âŒ è­¦å‘Šï¼šæœªæ‰¾åˆ° E2B_API_KEY")
else:
    print("âœ… E2B Key å·²åŠ è½½")


# ============================================================================
# LLM Client (Gemini REST API)
# ============================================================================

def call_llm(system_prompt: str, user_prompt: str) -> str:
    """
    Calls Google Gemini API using standard library urllib (Zero dependencies).
    Merges system prompt into user prompt as per instructions.
    """
    api_key = os.environ.get(API_KEY_ENV)
    if not api_key:
        print(f"Error: Environment variable {API_KEY_ENV} not set.")
        sys.exit(1)

    model = os.environ.get(MODEL_ENV, DEFAULT_MODEL)
    
    # Gemini API Endpoint
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}"
    
    # Merge prompts (Gemini REST API doesn't strictly distinguish system prompt in the simplest payload)
    # We'll use the pattern: System Instruction \n\n User Instruction
    full_prompt = f"{system_prompt}\n\n{user_prompt}"

    payload = {
        "contents": [{
            "parts": [{"text": full_prompt}]
        }],
        "generationConfig": {
            "temperature": 0.7
        }
    }

    data = json.dumps(payload).encode('utf-8')
    req = urllib.request.Request(url, data=data, headers={'Content-Type': 'application/json'})

    try:
        with urllib.request.urlopen(req) as response:
            result_json = json.loads(response.read().decode('utf-8'))
            
            # Parse Gemini Response
            try:
                candidates = result_json.get('candidates', [])
                if not candidates:
                    raise ValueError("No candidates returned")
                
                content = candidates[0].get('content', {})
                parts = content.get('parts', [])
                if not parts:
                    raise ValueError("No parts returned")
                
                return parts[0].get('text', "")
            except Exception as e:
                return f"Error parsing response: {str(e)} | Raw: {json.dumps(result_json)}"

    except urllib.error.HTTPError as e:
        error_body = e.read().decode('utf-8')
        print(f"\n[LLM Error] HTTP {e.code}: {error_body}")
        sys.exit(1)
    except Exception as e:
        print(f"\n[LLM Error] {str(e)}")
        sys.exit(1)

# ============================================================================
# Helper Functions (Cleaners & logic from YAML)
# ============================================================================

def safe_json_decode(raw_str):
    try:
        return json.loads(f'"{raw_str}"')
    except:
        return raw_str

def remove_trailing_slash(content):
    content = content.strip()
    while content.endswith("/n") or content.endswith("\\n"):
        if content.endswith("/n"):
            content = content[:-2]
        elif content.endswith("\\n"):
            content = content[:-2]
    return content.strip()

def extract_markdown_code(text: str) -> str:
    """Extract code from markdown code blocks."""
    text = text.strip()
    pattern = r"```(?:python|json)?\s*(.*?)```"
    match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
    if match:
        return match.group(1).strip()
    return text

def cleaner_source_code(llm_output: str) -> str:
    """Logic from 'Code Cleaner (1)' node."""
    code = extract_markdown_code(llm_output)
    
    # Try to unwrap if it's a JSON wrapper (sometimes LLMs wrap code in JSON)
    try:
        data = json.loads(code)
        if isinstance(data, dict) and "code" in data:
            return data["code"]
    except json.JSONDecodeError:
        pass
        
    return code.strip("`")

def cleaner_test_case(llm_output: str) -> str:
    """Logic from 'Code Cleaner (2)' node."""
    code = extract_markdown_code(llm_output)
    
    # Logic to parse the specific JSON format output by Testcase Agent
    # It expects a list of dicts with 'suffix', 'content'
    extracted_code = ""
    
    try:
        # Try pure JSON load first
        data = json.loads(code)
    except:
        # If strict JSON fails, try the regex decode fallback from YAML
        # (Simplified here to just look for content pattern as the regex in YAML was complex)
        # However, for robustness, let's assume the LLM follows instructions reasonably well
        # or that we can fix simple JSON errors.
        print("  [Cleaner] Warning: JSON decode failed for testcase, trying simple extraction...")
        # Fallback: simple text extraction if it looks like code
        if "class " in code and "unittest" in code:
            return code
        return "# Error parsing test case JSON"

    if isinstance(data, dict):
        data = [data]
        
    if isinstance(data, list):
        for obj in data:
            if obj.get("suffix") == "py":
                obj_code = obj.get("content", "")
                obj_code = remove_trailing_slash(obj_code)
                extracted_code += obj_code + "\n\n"
    
    return extracted_code.strip()

def cleaner_debug_agent(llm_output: str) -> Dict[str, str]:
    """Logic from 'Code Cleaner' (Node 1766771232701) handling Debugger output."""
    code = extract_markdown_code(llm_output)
    
    try:
        update = json.loads(code)
    except:
        # Regex fallback from YAML
        pair_pattern = (
            r'"target_file"\s*:\s*"(.*?)(?<!\\)"'
            r'.*?'
            r'"file_content"\s*:\s*"(.*?)(?<!\\)"'
        )
        match = re.search(pair_pattern, code, re.DOTALL)
        if match:
            raw_target = match.group(1)
            raw_content = match.group(2)
            return {
                "target_file": safe_json_decode(raw_target),
                "file_content": remove_trailing_slash(safe_json_decode(raw_content))
            }
        return {"target_file": "ERROR", "file_content": ""}

    return {
        "target_file": update.get("target_file"),
        "file_content": update.get("file_content")
    }

# ============================================================================
# Tool: Code Execution (Subprocess)
# ============================================================================

def execute_code(source_code: str, test_code: str) -> Dict[str, Any]:
    """
    Simulates the Vercel Code Runner.
    Writes code to temp files and runs unittest.
    """

    with Sandbox.create(api_key=E2B_API_KEY) as sandbox:
        print("ğŸš€ æ²™ç®±å·²å¯åŠ¨...")

        sandbox.files.write("main.py", source_code)
        print("âœ… æ–‡ä»¶ main.py å·²å†™å…¥æ²™ç®±")

        test_file_content = "import unittest\nfrom main import *\n\n" + test_code

        sandbox.files.write("test.py", test_file_content)
        print("âœ… æ–‡ä»¶ test.py å·²å†™å…¥æ²™ç®±")

        # proc = sandbox.commands.run("python test.py")

        # 3. æ‰§è¡Œä»£ç  (æ›¿ä»£ subprocess çš„éƒ¨åˆ†)
        # æ³¨æ„ï¼štimeout å‚æ•°ç›´æ¥åœ¨è¿™é‡Œè®¾ç½®ï¼Œå•ä½æ˜¯ç§’
        try:
            proc = sandbox.commands.run("python test.py", timeout=30)
            
            # E2B çš„ proc å¯¹è±¡ç›´æ¥æä¾›äº† exit_code, stdout, stderr
            is_pass = proc.exit_code == 0
            
            return {
                "is_pass": is_pass,
                "stderr": proc.stderr,
                "stdout": proc.stdout,
                # å¦‚æœæµ‹è¯•é€šè¿‡ï¼Œerror ä¸ºç©ºï¼›å¦åˆ™æç¤º Failed
                "error": "" if is_pass else "Tests failed"
            }

        except TimeoutError:
            # E2B å†…éƒ¨è¶…æ—¶ä¼šæŠ›å‡º TimeoutError
            return {
                "is_pass": False,
                "stderr": "Execution Timed Out (30s)",
                "stdout": "",
                "error": "Timeout"
            }
        except Exception as e:
            # æ•è·æ‰§è¡Œè¿‡ç¨‹ä¸­çš„å…¶ä»–é”™è¯¯ï¼ˆå¦‚æ²™ç®±å†…éƒ¨å´©æºƒç­‰ï¼‰
            return {
                "is_pass": False,
                "stderr": str(e),
                "stdout": "",
                "error": "Execution Error during run"
            }


def qa_judge(exec_result: Dict[str, Any]) -> Tuple[bool, str]:
    """Logic from 'QA Judge' node."""
    is_pass = exec_result.get("is_pass", False)
    error_log = ""
    
    if not is_pass:
        error_log = f"Summary: Tests Failed\n"
        error_log += f"Details:\n{exec_result.get('stderr')}\n"
        if exec_result.get('error'):
             error_log += f"Traceback:\n{exec_result.get('error')}"
             
    return is_pass, error_log

# ============================================================================
# Prompts (Hardcoded from YAML)
# ============================================================================

PROMPT_PM_SYSTEM = """# Role
ä½ æ˜¯ä¸€ä¸ª**æŠ€æœ¯äº§å“ç»ç† (Technical Product Manager)**ã€‚
ä½ çš„ç‰¹é•¿æ˜¯å¹³è¡¡ä¸šåŠ¡éœ€æ±‚ä¸æŠ€æœ¯çº¦æŸã€‚ä½ è´Ÿè´£ç»™å¼€å‘å›¢é˜Ÿï¼ˆCoder Agentï¼‰å’Œæµ‹è¯•å›¢é˜Ÿï¼ˆTest Agentï¼‰è¾“å‡ºç²¾å‡†çš„éœ€æ±‚æ–‡æ¡£ã€‚

# Task
åˆ†æç”¨æˆ·è¾“å…¥ ã€‚
1.  **è¯†åˆ«ç¡¬æ€§çº¦æŸï¼š** æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æŒ‡å®šäº†å…·ä½“çš„çº¦æŸï¼Œå¦‚**æ•°æ®ç»“æ„**ï¼ˆå¦‚å­—å…¸ã€åˆ—è¡¨ï¼‰ã€**å˜é‡å**ã€**å‡½æ•°ç­¾å**æˆ–**è¾“å…¥è¾“å‡ºç±»å‹**ç­‰ã€‚
2.  **å¡«å……ä¸šåŠ¡ç©ºç™½ï¼š** å¯¹äºç”¨æˆ·æœªæåŠçš„ä¸šåŠ¡é€»è¾‘ï¼ˆå¦‚å¼‚å¸¸å¤„ç†ã€è¾¹ç•Œæƒ…å†µï¼‰ï¼Œè¿›è¡Œåˆç†çš„è¡¥å……å’Œå®Œå–„ã€‚
3.  **ç”Ÿæˆæ–‡æ¡£ï¼š** è¾“å‡ºä¸€ä»½æ—¢åŒ…å«ä¸šåŠ¡æµç¨‹ï¼Œåˆä¸¥æ ¼éµå®ˆç”¨æˆ·æŠ€æœ¯æŒ‡å®šçš„éœ€æ±‚æ–‡æ¡£ã€‚

# Critical Rules (The "Constitution")
1.  **æŠ€æœ¯çº¦æŸä¸å¯ä¾µçŠ¯ï¼š**
    * å¦‚æœç”¨æˆ·è¯´ "è¾“å…¥å¿…é¡»æ˜¯ `inventory: dict`"ï¼Œä½ **å¿…é¡»**å°†å…¶åˆ—ä¸ºç¡¬æ€§çº¦æŸã€‚
    * **ä¸¥ç¦** ä¿®æ”¹ç”¨æˆ·çš„å®šä¹‰ï¼Œä¾‹å¦‚å°†ç”¨æˆ·æŒ‡å®šçš„ `dict` ç±»å‹æ”¹ä¸º `class`ç±»å‹ï¼Œæˆ–ä¿®æ”¹ç”¨æˆ·æŒ‡å®šçš„å­—æ®µåç­‰ã€‚
2.  **ä¸šåŠ¡é€»è¾‘è¦å…·ä½“ï¼š**
    * å³ä½¿æŠ€æœ¯çº¦æŸå¾ˆå…·ä½“ï¼Œä½ ä¾ç„¶è¦æè¿°â€œé€»è¾‘æµâ€ã€‚ä¾‹å¦‚ç”¨æˆ·å®šä¹‰äº†å‡½æ•°æ¥å£ï¼Œä½ è¦è¡¥å……â€œåº“å­˜ä¸è¶³æ—¶è¯¥å‡½æ•°å…·ä½“æ€ä¹ˆåšâ€ã€‚
3.  **ä¸è¦å†™ä»£ç ï¼š** ä¾ç„¶ä¿æŒç”¨è‡ªç„¶è¯­è¨€æˆ–ä¼ªä»£ç æè¿°ï¼Œä¸è¦ç›´æ¥å†™ Python å®ç°ã€‚

# Output Format (Structured Markdown)

## 1. ğŸ¯ Project Overview
* **ç›®æ ‡ï¼š** ä¸€å¥è¯æ¦‚æ‹¬ç³»ç»ŸåŠŸèƒ½ã€‚

## 2. ğŸ” Technical Constraints (ç”¨æˆ·æŒ‡å®šçš„æŠ€æœ¯çº¦æŸ)
* *æ³¨æ„ï¼šä»…å½“ç”¨æˆ·åœ¨è¾“å…¥ä¸­æ˜ç¡®æŒ‡å®šäº†æŠ€æœ¯ç»†èŠ‚æ—¶å¡«å†™æ­¤éƒ¨åˆ†ã€‚å¦‚æœç”¨æˆ·æ²¡è¯´ï¼Œå†™ "None (ç”± Coder è‡ªç”±å‘æŒ¥)"ã€‚*
* **æ•°æ®ç»“æ„çº¦æŸï¼š** (ä¾‹å¦‚ï¼šUser æŒ‡å®š `orders` å¿…é¡»æ˜¯ `List[Dict]`)
* **æ¥å£ç­¾åçº¦æŸï¼š** (ä¾‹å¦‚ï¼šUser æŒ‡å®šå‡½æ•°åä¸º `process_orders`ï¼Œè¿”å› `tuple`)
* **å­—æ®µå‘½åçº¦æŸï¼š** (ä¾‹å¦‚ï¼šå¿…é¡»åŒ…å« `qty` å­—æ®µ)
* **å…¶ä»–ç”¨æˆ·è‡ªå®šä¹‰çš„çº¦æŸï¼š** ç”¨æˆ·åœ¨éœ€æ±‚é‡Œæå‡ºçš„çº¦æŸå¿…é¡»å…¨éƒ¨å†™å‡ºæ¥ï¼Œä¸èƒ½é—æ¼

## 3. ğŸŒŠ Business Logic Flow (ä¸šåŠ¡é€»è¾‘æµ)
* *è¿™æ˜¯ç»™ Coder çš„é€»è¾‘ä¼ªä»£ç æŒ‡å¼•ã€‚*
* *è¯·æ ¹æ®å®é™…é€»è¾‘å¤æ‚åº¦åˆ—å‡ºæ­¥éª¤ï¼Œä¸é™æ•°é‡ã€‚*
1.  **Step 1:** ...
2.  **Step 2:** ...
3.  **Step 3:** ...ï¼Œå¯ä»¥æœ‰æ›´å¤šæ­¥éª¤

## 4. âœ… Acceptance Criteria (éªŒæ”¶æ ‡å‡†)
* *åˆ—å‡ºæ‰€æœ‰å¿…é¡»æ»¡è¶³çš„éªŒæ”¶æ¡ä»¶ï¼Œæ¶µç›–æ­£å¸¸è·¯å¾„å’Œè¾¹ç¼˜æƒ…å†µã€‚*
* **AC1:** ... 
* **AC2:** ... 
* **... (Add more as needed)**

---"""

PROMPT_CODER_SYSTEM = """ä½ æ˜¯ Coder Agentã€‚è¯·æ ¹æ®ç”¨æˆ·çš„user story
ç¼–å†™å®Œæ•´çš„ã€å¯è¿è¡Œçš„ä»£ç ã€‚

ä¸¥æ ¼éµå®ˆä»¥ä¸‹è¦æ±‚ï¼š
1. åªä½¿ç”¨ Python æ ‡å‡†åº“ï¼ˆé™¤éè§„èŒƒä¸­æ˜ç¡®å…è®¸ç¬¬ä¸‰æ–¹åº“ï¼‰
2. å¿…é¡»æ˜¯å•æ–‡ä»¶ç¨‹åº
3. å¿…é¡»åŒ…å«æ¸…æ™°çš„ç¨‹åºå…¥å£ï¼ˆif __name__ == "__main__":ï¼‰
4. ç¨‹åºå¯ä»¥ç›´æ¥é€šè¿‡ `python main.py`ï¼ˆæˆ–ç­‰ä»·æ–¹å¼ï¼‰è¿è¡Œ
5. ä¸è¦å®ç°è§„èŒƒä¸­æ˜ç¡®æ ‡æ³¨ä¸º NON_GOALS çš„å†…å®¹

è¾“å‡ºè¦æ±‚ï¼š
- åªè¾“å‡ºå®Œæ•´ä»£ç 
- ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šã€æ³¨é‡Šè¯´æ˜æˆ– Markdown æ ‡è®°
"""

PROMPT_TESTCASE_SYSTEM = """# Role
ä½ æ˜¯ä¸€ä¸ª Python QA æµ‹è¯•ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸ºç»™å®šçš„ä»£ç ç¼–å†™ `unittest` æµ‹è¯•ç”¨ä¾‹ã€‚

# Inputs
ä½ æ‹¥æœ‰ä»¥ä¸‹è¾“å…¥ï¼š
1. **User Story (ç”¨æˆ·éœ€æ±‚)** 
2. **Source Code (ä¸šåŠ¡ä»£ç ):** - è¿™æ˜¯ä¸Šä¸€æ­¥ç”Ÿæˆçš„ä¸šåŠ¡é€»è¾‘ä»£ç ã€‚

# Execution Context (CRITICAL)
ä½ çš„æµ‹è¯•ä»£ç å°†åœ¨ä¸€ä¸ª**å…±äº«å†…å­˜ç¯å¢ƒ**ä¸­è¿è¡Œï¼š
1. **NO IMPORTS:** å‡è®¾ `Source Code` ä¸­çš„å‡½æ•°å’Œç±»**å·²ç»å®šä¹‰åœ¨å½“å‰ç¯å¢ƒ**ä¸­ã€‚ä¸è¦å¯¼å…¥ä»»ä½•`Source Code`ä¸­çš„åŒ…ï¼Œç›´æ¥è°ƒç”¨ä¸šåŠ¡å‡½æ•°å³å¯ã€‚
   - ä¾‹å¦‚ï¼šå¦‚æœæºä»£ç é‡Œå®šä¹‰äº† `def add(a,b):`ï¼Œä½ çš„æµ‹è¯•é‡Œç›´æ¥å†™ `add(1, 2)`ã€‚
2. **Library Imports:** ä½ ä¾ç„¶éœ€è¦å¯¼å…¥ `unittest` å’Œ `unittest.mock`ï¼ˆå¦‚æœéœ€è¦ï¼‰ã€‚

RULE: NO MENTAL MATH (ç¦æ­¢å¿ƒç®—) When writing assertEqual, DO NOT calculate the expected value yourself. Write the mathematical expression and let Python calculate it.
âŒ Bad: self.assertEqual(result, 27) (You might calculate wrong)
âœ… Good: self.assertEqual(result, 12 + (10 - 5) * 3) (Safe & Accurate)

RULE: CHECK STATE SIDE-EFFECTS (æ£€æŸ¥çŠ¶æ€å‰¯ä½œç”¨)
Read the Source Code to see if a method modifies class attributes (e.g., `self.count -= 1`, `self.data.clear()`, or `self.status = False`).
If a method "consumes" a resource or changes a status, do not assume you can call it repeatedly with the same result.
âŒ Bad: Calling a method in a loop assuming it always returns True (it might have used up a quota or cleared a list).
âœ… Good: If code shows `self.quota -= 1`, your test must handle the case where quota runs out (expect False or Exception).

RULE: MANDATORY THOUGHT TRACE (å¼ºåˆ¶æ€ç»´é“¾)
You are NOT allowed to write the test code directly.
Inside your JSON object, you MUST include a field named `reasoning` **BEFORE** the `content` field.
In this `reasoning` field, you must write a short paragraph where you:
1.  **List State Variables:** Identify variables in Source Code that change (e.g., `self.balance`, `self.inventory`).
2.  **Trace Side Effects:** Explicitly state what happens to these variables after a method call (e.g., "After purchase, balance resets to 0").
3.  **Plan Reset Strategy:** If writing a loop or sequential test, decide when to re-initialize the state by **Simulate Code Execution**. You should simulate code execution as if you are the compiler or interpreter.
4.  **Evaluate Testcase Outcome:** Use reasoning to derive your steps that leads you to an outcome.

*The quality of your code depends on this analysis.*

Rule: Explicit Variable Expansion & Constraint Matching
When you use a variable (e.g., total_weight, message_length) as an argument for a function that enforces Strict Input Constraints (e.g., specific allowed values or size limits), you must perform a "Trace & Verify" step in your reasoning.
Steps:
Trace Value: What is the actual value of the variable in this specific test context?
Verify: Does this value exist in the function's allowed input list?
Decompose: If the value is valid logic-wise (e.g., total amount) but invalid structure-wise (e.g., wrong block size), you MUST break it down into a loop or sequence of valid calls.

# Workflow
1. **Analyze Code:** é˜…è¯» `Source Code`ï¼Œæå–ä¸»å‡½æ•°åæˆ–ç±»åï¼ˆä¾‹å¦‚ `solution` æˆ– `calculate_tax`ï¼‰ã€‚
2. **Verify Logic:** æ ¹æ® `User Story` ç†è§£é¢„æœŸçš„è¾“å…¥è¾“å‡ºã€‚
3. **Perform Reasoning (é€»è¾‘æ¨æ¼”):** ç»“åˆ `Source Code` å®ç°ç»†èŠ‚è¿›è¡Œæ·±åº¦åˆ†æï¼ŒåŒ…æ‹¬ä½† **ä¸é™äº**ï¼š * **State Tracking (çŠ¶æ€è¿½è¸ª):** è¯†åˆ«ä»£ç ä¸­çš„å‰¯ä½œç”¨ï¼ˆSide Effectsï¼‰ã€‚æ¯”å¦‚ï¼šæŸä¸ªæ–¹æ³•æ‰§è¡Œåæ˜¯å¦é‡ç½®äº† `self.balance` æˆ–æ¸…ç©ºäº†åˆ—è¡¨ï¼Ÿ * **Input Validation (è¾¹ç•Œæ£€æŸ¥):** æ£€æŸ¥ä»£ç ä¸­çš„ `if` æ¡ä»¶ï¼ˆå¦‚ `if x not in [1, 5, 10]`ï¼‰ï¼Œç¡®ä¿æµ‹è¯•è¾“å…¥çš„åˆæ³•æ€§ã€‚ * **Scenario Simulation (åœºæ™¯æ¨¡æ‹Ÿ):** åœ¨ç¼–å†™ä»£ç å‰ï¼Œå…ˆåœ¨è„‘æµ·ä¸­è¿è¡Œä¸€éå¾ªç¯é€»è¾‘ï¼Œåˆ¤æ–­ç¬¬äºŒæ¬¡è¿­ä»£æ˜¯å¦éœ€è¦é‡æ–°åˆå§‹åŒ–èµ„æºï¼ˆå¦‚é‡æ–°æŠ•å¸ï¼‰ã€‚

4. **Write Tests:** ç¼–å†™ä¸€ä¸ªç»§æ‰¿è‡ª `unittest.TestCase` çš„ç±»ã€‚
   - åŒ…å«æ­£å¸¸æƒ…å†µ (Happy Path)ã€‚
   - åŒ…å«è¾¹ç•Œæƒ…å†µ (Edge Cases)ã€‚

# Output Format (JSON)
è¯·è¾“å‡ºä¸”ä»…è¾“å‡ºä¸€ä¸ª JSON åˆ—è¡¨ï¼Œä»£è¡¨ä½ çš„æ‰€æœ‰æµ‹è¯•æ–‡ä»¶ï¼Œåˆ—è¡¨é‡Œçš„æ¯ä¸ªjson objectä»£è¡¨ä¸€ä¸ªæµ‹è¯•æ–‡ä»¶ã€‚å³ä½¿ä½ è§‰å¾—åªéœ€è¦ä¸€ä¸ªæµ‹è¯•æ–‡ä»¶ï¼Œä¹Ÿè¦ç”¨jsonåˆ—è¡¨æ¥åŒ…è£…

æ ¼å¼è¦æ±‚ï¼š
[
  {
      "reasoning": "...",
    "filename": "test.py",
    "suffix": "py",
    "content": "import unittest\\n\\nclass TestSolution(unittest.TestCase):\\n    def test_case_1(self):\\n        # ç›´æ¥è°ƒç”¨å‡½æ•°ï¼Œæ— éœ€å¯¼å…¥\\n        self.assertEqual(solution(1, 2), 3)",
    "type": "test"
  }
]

Constraints:
ä¸è¦åŒ…å« Markdown ä»£ç å—æ ‡è®°ï¼ˆå¦‚ ```pythonï¼‰ã€‚
ç¡®ä¿ JSON æ ¼å¼åˆæ³•ã€‚
ä¸è¦ä½¿ç”¨ if __name__ == '__main__':ã€‚
[JSON Formatting Rules]
NO UNESCAPED QUOTES: If you need to quote something inside the reasoning text, use Single Quotes (') or **Backticks ()**. Never use Double Quotes (") inside the JSON value unless they are escaped (\\").
"""

PROMPT_DEBUG_SYSTEM = """Role: You are an expert Python Debugger and Code Arbiter.
Goal: Analyze the provided Source Code, Test Case, User Story, and Execution Output (Error Logs) to fix the failure.
Decision Protocol (The Router Logic): You must decide which file contains the root cause of the failure.
High Priority (Fix Source Code): Default behavior. If the logic is wrong, the calculation is off, or the output format doesn't match the requirement, fix the Source Code.
Low Priority (Fix Test Case): ONLY fix the Test Case if:
The test uses specific variable names/function calls that do not exist in the source (Hallucination/NameError).
The test violates strict constraints defined in the `User Story`.
The test expectation is physically impossible or logically flawed.
Constraint: Do not change the Test Case just to make it pass. Only change it if it is objectively wrong.
Fixing Requirements:
Analyze: Think deeply about why it failed.
Minimal Changes: Make only the smallest necessary changes to fix the specific error.
No Refactoring: Do NOT change the overall architecture or class structure.
Consistency: Keep entry points (function names, class names) identical.
Conflict Resolution: If BOTH need fixing, choose to fix the TEST CASE first (to establish a correct standard for the next loop).

Output Format: You must output a SINGLE JSON Object. Do not output Markdown blocks outside the JSON.

{
  "reasoning": "Brief analysis of the error. Explicitly state WHY you chose to fix this specific file (e.g., 'The test fails because it tries to call a non-existent function `add` globally, but the source defines it inside `Addition` class.').",
  "target_file": "SOURCE",  // or "TEST", only those two.
  "file_content": "The FULL content of the fixed file (Source or Test)."
}

You may include simple comments noting what have you fixed in the `file_content` field of the JSON, but no extended thinking in comments. all thinking should be indicated in the `reasoning` field.

âŒ BAD COMMENTS (Strictly Prohibited): Do not write your "stream of consciousness" or "thinking process" in comments.
# I am thinking maybe I should change this loop...
# Let me try to see if using a stack works better...
# Wait, this logic might fail for negative numbers, let me reconsider...
# The previous code was 1+1=3, which is wrong. I am trying to find a way to make it 2. Let's try method A...
âœ… GOOD COMMENTS (Allowed): Write comments that explain the outcome or the reason for a specific fix.
# Fix: Corrected arithmetic logic (1+1=2).
# Note: This regex handles nested brackets.
# Bugfix: Previously, digits were incorrectly treated as multipliers only.
"""

# ============================================================================
# Main Agent Flow
# ============================================================================

def main():
    if len(sys.argv) < 2:
        print("Usage: python coding_agent.py \"Your project requirement\"")
        sys.exit(1)
        
    user_requirement = sys.argv[1]
    
    print(f"ğŸš€ CodingAgent Started")
    print(f"ğŸ“‹ Requirement: {user_requirement}\n")

    # 1. PM Agent
    print("ğŸ¤– [PM Agent] Analyzing requirements...")
    pm_response = call_llm(PROMPT_PM_SYSTEM, user_requirement)
    user_story = pm_response
    print("âœ… User Story Generated.\n")
    
    # 2. Initial Coder
    print("ğŸ‘¨â€ğŸ’» [Initial Coder] Writing code...")
    coder_response = call_llm(PROMPT_CODER_SYSTEM, user_story)
    current_code = cleaner_source_code(coder_response)
    print("âœ… Initial Code Generated.\n")

    # 3. Testcase Agent
    print("ğŸ§ª [Testcase Agent] Generating tests...")
    test_user_prompt = f"**User Story**: {user_story}\n**Source Code**: {current_code}"
    test_response = call_llm(PROMPT_TESTCASE_SYSTEM, test_user_prompt)
    current_testcase = cleaner_test_case(test_response)
    print("âœ… Test Cases Generated.\n")
    
    # Loop Configuration
    max_loops = 3
    loop_count = 0
    
    while loop_count < max_loops:
        loop_count += 1
        print(f"ğŸ”„ [Loop {loop_count}/{max_loops}] Executing & Testing...")
        
        # 4. Execute Code
        exec_result = execute_code(current_code, current_testcase)
        is_pass, error_log = qa_judge(exec_result)
        
        if is_pass:
            print("ğŸ‰ [QA Judge] Tests Passed!")
            break
        else:
            print("âŒ [QA Judge] Tests Failed.")
            print(f"   Error Summary: {error_log.splitlines()[0] if error_log else 'Unknown'}")
            
            if loop_count == max_loops:
                print("âš ï¸ Max loops reached. Exiting with last version.")
                break
            
            # 5. Debug Agent
            print("ğŸ”§ [Debug Agent] Analyzing failure...")
            debug_prompt = (
                f"**Source Code**: {current_code}\n\n"
                f"**Test Case**: {current_testcase}\n\n"
                f"**User Story**: {user_story}\n\n"
                f"**Execution Output**: {error_log}\n"
            )
            debug_response = call_llm(PROMPT_DEBUG_SYSTEM, debug_prompt)
            debug_fix = cleaner_debug_agent(debug_response)
            
            target = debug_fix.get("target_file")
            content = debug_fix.get("file_content")
            
            if target == "SOURCE":
                print("ğŸ› ï¸  Fixing Source Code...")
                current_code = content
            elif target == "TEST":
                print("ğŸ› ï¸  Fixing Test Case...")
                current_testcase = content
            else:
                print("âš ï¸ Debug Agent returned unknown target. Stopping.")
                break
                
    # End
    output_dir = "output"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    final_main_path = os.path.join(output_dir, "main.py")
    final_test_path = os.path.join(output_dir, "test_generated.py")
    
    with open(final_main_path, "w", encoding="utf-8") as f:
        f.write(current_code)
        
    with open(final_test_path, "w", encoding="utf-8") as f:
        f.write(f"import unittest\nfrom main import *\n\n{current_testcase}")
        
    print("\nâœ¨ Process Completed!")
    print(f"ğŸ“‚ Final Code: {final_main_path}")
    print(f"ğŸ“‚ Final Test: {final_test_path}")

if __name__ == "__main__":
    main()
