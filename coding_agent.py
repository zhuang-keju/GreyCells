
import os
import sys
import json
import re
import subprocess
import time
import argparse
import urllib.request
import urllib.error
from typing import Dict, Any, Tuple, Optional
from markdown_it import MarkdownIt



from e2b_code_interpreter import Sandbox
from dotenv import load_dotenv



# ============================================================================
# Configuration & Constants
# ============================================================================

load_dotenv()

DEFAULT_MODEL = "gemini-2.0-flash"  # Default if env var not set
API_KEY_ENV = "LLM_API_KEY"
PROVIDER_ENV = "LLM_PROVIDER"
MODEL_ENV = "LLM_MODEL"
E2B_API_KEY = os.environ.get("E2B_API_KEY")

if not E2B_API_KEY:
    print("âŒ è­¦å‘Šï¼šæœªæ‰¾åˆ° E2B_API_KEY")
else:
    print("âœ… E2B Key å·²åŠ è½½")


# ============================================================================
# LLM Client (Gemini REST API)
# ============================================================================

def call_llm(system_prompt: str, user_prompt: str, current_task_stats = None) -> str:
    """
    Calls Google Gemini API using standard library urllib (Zero dependencies).
    Merges system prompt into user prompt as per instructions.
    """
    api_key = os.environ.get(API_KEY_ENV)
    if not api_key:
        print(f"Error: Environment variable {API_KEY_ENV} not set.")
        sys.exit(1)

    model = os.environ.get(MODEL_ENV, DEFAULT_MODEL)
    
    # Gemini API Endpoint
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}"
    
    # Merge prompts (Gemini REST API doesn't strictly distinguish system prompt in the simplest payload)
    # We'll use the pattern: System Instruction \n\n User Instruction
    full_prompt = f"{system_prompt}\n\n{user_prompt}"

    payload = {
        "contents": [{
            "parts": [{"text": full_prompt}]
        }],
        "generationConfig": {
            "temperature": 0.7
        }
    }

    data = json.dumps(payload).encode('utf-8')
    req = urllib.request.Request(url, data=data, headers={'Content-Type': 'application/json'})

    try:
        with urllib.request.urlopen(req) as response:
            result_json = json.loads(response.read().decode('utf-8'))
            
            # Parse Gemini Response
            try:
                candidates = result_json.get('candidates', [])
                if not candidates:
                    raise ValueError("No candidates returned")
                if current_task_stats is not None:
                    usage = result_json.get('usageMetadata', {})
                    total_tokens = usage.get('totalTokenCount', 0)
                    current_task_stats["tokens"] += total_tokens
                    current_task_stats["calls"] += 1
                content = candidates[0].get('content', {})
                parts = content.get('parts', [])
                if not parts:
                    raise ValueError("No parts returned")
                
                return parts[0].get('text', "")
            except Exception as e:
                return f"Error parsing response: {str(e)} | Raw: {json.dumps(result_json)}"

    except urllib.error.HTTPError as e:
        error_body = e.read().decode('utf-8')
        print(f"\n[LLM Error] HTTP {e.code}: {error_body}")
        sys.exit(1)
    except Exception as e:
        print(f"\n[LLM Error] {str(e)}")
        sys.exit(1)

# ============================================================================
# Helper Functions (Cleaners & logic from YAML)
# ============================================================================

def safe_json_decode(raw_str):
    try:
        return json.loads(f'"{raw_str}"')
    except:
        return raw_str

def remove_trailing_slash(content):
    content = content.strip()
    while content.endswith("/n") or content.endswith("\\n"):
        if content.endswith("/n"):
            content = content[:-2]
        elif content.endswith("\\n"):
            content = content[:-2]
    return content.strip()

def extract_markdown_code(text: str) -> str:
    """Extract code from markdown code blocks."""
    text = text.strip()
    pattern = r"```(?:python|json)?\s*(.*?)```"
    match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
    if match:
        return match.group(1).strip()
    return text



def validate_test_quality(original_test_code, new_test_code):
    """
    å®ªå…µï¼šæ£€æŸ¥ Executor æ˜¯å¦æŠŠ PBT åˆ äº†ã€‚
    """
    # å…³é”®è¯ç‰¹å¾ (æ ¹æ®ä½ çš„ PBT é£æ ¼è°ƒæ•´)
    pbt_keywords = ["random.", "hypothesis", "for _ in range", "for i in range"]
    
    # 1. æ£€æŸ¥æ–°ä»£ç é‡Œæœ‰æ²¡æœ‰ PBT ç‰¹å¾
    has_pbt = any(k in new_test_code for k in pbt_keywords)
    
    if not has_pbt:
        # å¦‚æœåŸä»£ç é‡Œæœ‰ï¼Œæ–°ä»£ç é‡Œæ²¡äº† -> æŠ¥è­¦ï¼
        if any(k in original_test_code for k in pbt_keywords):
            return False, "âš ï¸ Error: You removed the Property-Based Testing (random/loop) logic! Restore it immediately."
            
    return True, "Pass"


# extract one json object only
def extract_json_regex(text: str, schema: Dict[str, str]) -> Dict[str, Any]:
    """
    åŸºäºé¢„å®šä¹‰ç±»å‹çš„é€šç”¨æ­£åˆ™æå–å™¨ã€‚
    
    Args:
        text: LLM è¿”å›çš„åŸå§‹æ–‡æœ¬
        schema: å­—æ®µååˆ°ç±»å‹çš„æ˜ å°„ï¼Œæ”¯æŒ 'string', 'list', 'bool'
                ä¾‹å¦‚: {"filename": "string", "packages": "list"}
    
    Returns:
        æå–åˆ°çš„æ•°æ®å­—å…¸ã€‚æœªåŒ¹é…åˆ°çš„å­—æ®µå°†æ ¹æ®ç±»å‹è¿”å›é»˜è®¤å€¼ (ç©ºå­—ç¬¦ä¸²æˆ–ç©ºåˆ—è¡¨)ã€‚
    """
    result = {}
    
    for field, field_type in schema.items():
        if field_type == "string":
            # åŒ¹é…å­—ç¬¦ä¸²: "key": "value"
            # (?<!\\)" ç¡®ä¿ä¸åŒ¹é…è½¬ä¹‰çš„å¼•å·
            pattern = fr'"{field}"\s*:\s*"(.*?)(?<!\\)"'
            match = re.search(pattern, text, re.DOTALL)
            if match:
                result[field] = safe_json_decode(match.group(1))
            else:
                result[field] = ""
                
        elif field_type == "list":
            # åŒ¹é…åˆ—è¡¨: "key": [...]
            # (.*?) éè´ªå©ªåŒ¹é…ç›´åˆ°é‡åˆ°é—­åˆçš„ ]
            pattern = fr'"{field}"\s*:\s*\[(.*?)\]'
            match = re.search(pattern, text, re.DOTALL)
            if match:
                raw_content = match.group(1)
                # æå–åˆ—è¡¨å†…çš„æ‰€æœ‰å­—ç¬¦ä¸²é¡¹ (æ”¯æŒåŒå¼•å·å’Œå•å¼•å·)
                items = re.findall(r'["\']([^"\']+)["\']', raw_content)
                # å¯¹æ¯ä¸€é¡¹è¿›è¡Œè§£ç 
                result[field] = [safe_json_decode(item) for item in items]
            else:
                result[field] = []

        elif field_type == "bool":
            # åŒ¹é…å¸ƒå°”å€¼: "key": true/false
            pattern = fr'"{field}"\s*:\s*(true|false)'
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                result[field] = match.group(1).lower() == "true"
            else:
                result[field] = False # é»˜è®¤å€¼
                
    return result




def parse_markdown_with_schema(llm_output: str, schema: dict) -> dict:
    """
    é€šç”¨ Markdown è§£æå™¨ (åŸºäº AST)ã€‚
    
    Args:
        llm_output (str): LLM è¾“å‡ºçš„åŸå§‹ Markdown æ–‡æœ¬ã€‚
        schema (dict): æå–è§„åˆ™é…ç½®ã€‚
            æ ¼å¼: { "HeaderName": ("target_key", "type") }
            - HeaderName: Markdown ä¸­çš„æ ‡é¢˜ (ä¸åŒºåˆ†å¤§å°å†™).
            - target_key: è¾“å‡ºå­—å…¸ä¸­çš„ key.
            - type: æå–æ¨¡å¼ ("text" | "code" | "json").
            
    Returns:
        dict: è§£æåçš„ç»“æœå­—å…¸ã€‚
    """
    md = MarkdownIt("commonmark", {"breaks": True, "html": True})
    tokens = md.parse(llm_output)
    

    # === ğŸ›¡ï¸ æ ¸å¿ƒä¿®å¤ï¼šæ£€æµ‹æ˜¯å¦è¢«å¥—äº†ä¸€å±‚ Markdown å¤–å£³ ===
    # é€»è¾‘ï¼šå¦‚æœ Token å¾ˆå°‘ï¼ˆé€šå¸¸åªæœ‰1ä¸ª fenceï¼‰ï¼Œä¸”è¿™ä¸ª fence çš„å†…å®¹é‡ŒåŒ…å« "## Reasoning"ï¼Œè¯´æ˜è¢«å¥—å¨ƒäº†
    if len(tokens) == 1 and tokens[0].type == 'fence':
        inner_content = tokens[0].content
        # ç®€å•çš„å—…æ¢ï¼Œçœ‹é‡Œé¢æœ‰æ²¡æœ‰æˆ‘ä»¬çš„å…³é”®è¯
        if "## Reasoning" in inner_content or "## Content" in inner_content:
            print("âš ï¸ Detected nested Markdown wrapper. Peeling it off and re-parsing...")
            return parse_markdown_with_schema(inner_content, schema) # <--- é€’å½’è°ƒç”¨ï¼


    # åˆå§‹åŒ–ç»“æœå­—å…¸
    result = {config[0]: None for config in schema.values()}
    
    # çŠ¶æ€æœºå˜é‡
    current_target_key = None
    current_type = None
    
    # é¢„å¤„ç† Schema: å»ºç«‹ "å°å†™æ ‡é¢˜ -> (target_key, type)" çš„æ˜ å°„ï¼Œå®ç°æ ‡é¢˜å¤§å°å†™ä¸æ•æ„Ÿ
    normalized_schema = {k.lower(): v for k, v in schema.items()}

    for i, token in enumerate(tokens):
        # 1. å‘ç°æ ‡é¢˜ (H1-H6)
        if token.type == 'heading_open':
            # è·å–æ ‡é¢˜æ–‡æœ¬ (heading_open çš„ä¸‹ä¸€ä¸ª token å¿…ç„¶æ˜¯ inline)
            # è¿™æ˜¯ä¸€ä¸ª Look-ahead æ“ä½œ
            if i + 1 < len(tokens) and tokens[i+1].type == 'inline':
                title_text = tokens[i+1].content.strip().lower()
                
                # æ£€æŸ¥è¿™ä¸ªæ ‡é¢˜æ˜¯å¦åœ¨æˆ‘ä»¬çš„ Schema é‡Œ
                if title_text in normalized_schema:
                    target_key, extract_type = normalized_schema[title_text]
                    current_target_key = target_key
                    current_type = extract_type
                else:
                    # é‡åˆ°äº†ä¸åœ¨ Schema é‡Œçš„æ ‡é¢˜ï¼Œåœæ­¢å½“å‰æ”¶é›†
                    current_target_key = None
            continue

        # 2. æ”¶é›†å†…å®¹ (æ ¹æ®å½“å‰çš„ target_key å’Œ type)
        if current_target_key:
            
            # === æ¨¡å¼ A: æå–çº¯æ–‡æœ¬ (Reasoning) ===
            if current_type == "text":
                # æ”¶é›†æ®µè½æ–‡æœ¬ã€è¡Œå†…æ–‡æœ¬
                if token.type == 'inline':
                    content = token.content
                    # åˆå§‹åŒ–æˆ–è¿½åŠ 
                    if result[current_target_key] is None:
                        result[current_target_key] = content
                    else:
                        result[current_target_key] += "\n" + content
            
            # === æ¨¡å¼ B: æå–ä»£ç å— (Content) ===
            elif current_type == "code":
                if token.type == 'fence':
                    # ç›´æ¥æå–ä»£ç å—å†…å®¹ï¼Œå¿½ç•¥è¯­è¨€æ ‡è®°
                    result[current_target_key] = token.content.strip()
            
            # === æ¨¡å¼ C: æå– JSON (Metadata) ===
            elif current_type == "json":
                if token.type == 'fence':
                    try:
                        # å°è¯•è§£æ JSON
                        json_content = json.loads(token.content.strip())
                        result[current_target_key] = json_content
                    except json.JSONDecodeError:
                        print(f"âš ï¸ è§£æé”™è¯¯: æ ‡é¢˜ '{current_target_key}' ä¸‹çš„ä»£ç å—ä¸æ˜¯åˆæ³• JSONã€‚")
                        result[current_target_key] = {} # å¤±è´¥è¿”å›ç©ºå­—å…¸

    return result



def cleaner_source_code(llm_output: str) -> dict:
    """Logic from 'Code Cleaner (1)' node."""
    print("=============================================")
    llm_output = llm_output.strip("\n`markdown") + "\n```"
    # print(llm_output)

    print("=============================================")


    coder_schema = {
    "Reasoning": ("reasoning", "text"),  # æå– ## Reasoning ä¸‹çš„æ–‡æœ¬ -> result['reasoning']
    "Content":   ("content", "code"),    # æå– ## Content ä¸‹çš„ä»£ç å— -> result['content']
    "Metadata":  ("metadata", "json")    # æå– ## Metadata ä¸‹çš„JSON -> result['metadata']
    }

    print(f"ğŸ§¹ [Cleaner] Parsing Coder Output...")
    cleaned_data = parse_markdown_with_schema(llm_output, coder_schema)
    meta = cleaned_data.get("metadata", {})
    source_code = cleaned_data.get("content")
    if not meta or not source_code:
        with open("logs.txt", "a") as f:
            f.write("================================================\n" * 2+llm_output)
    print(meta)
    print(source_code[:100])
    # if not meta:
        # print("=============================================")
        # print(llm_output)
        # print("=============================================")

    meta["content"] = source_code
    return meta




def cleaner_test_case(llm_output: str) -> dict:
    """Logic from 'Test Cleaner' node."""
    print("=============================================")
    llm_output = llm_output.strip("\n`markdown") + "\n```"
    # print(llm_output)

    print("=============================================")

    # 1. å®šä¹‰ Schema (å¯¹åº” Testcase Agent Prompt çš„ Markdown æ ‡é¢˜)
    test_schema = {
        "Reasoning": ("reasoning", "text"),  # æå– ## Reasoning ä¸‹çš„æµ‹è¯•ç­–ç•¥
        "Content":   ("content", "code"),    # æå– ## Content ä¸‹çš„æµ‹è¯•ä»£ç 
        "Metadata":  ("metadata", "json")    # æå– ## Metadata ä¸‹çš„å…ƒæ•°æ®
    }

    print(f"ğŸ§¹ [Cleaner] Parsing Test Case Output...")
    
    # 2. è°ƒç”¨é€šç”¨è§£æå™¨ (ä¸Šä¸€è½®å®ç°çš„ parse_markdown_with_schema)
    cleaned_data = parse_markdown_with_schema(llm_output, test_schema)
    
    # 3. æå–æ•°æ®
    meta = cleaned_data.get("metadata", {})
    test_code = cleaned_data.get("content")
    reasoning = cleaned_data.get("reasoning")
    if not meta or not test_code or not reasoning:
        with open("logs.txt", "a") as f:
            f.write("================================================\n" * 2+llm_output)

    # if not meta:
        # print("=============================================")
        # print(llm_output)
        # print("=============================================")

    # 4. æ‰“å°è°ƒè¯•ä¿¡æ¯
    print(f"   Target File: {meta.get('filename', 'test.py')}")
    if test_code:
        # åªæ‰“å°å‰ 80 ä¸ªå­—ç¬¦é¢„è§ˆ
        print(f"   Code Preview: {test_code[:80].replace(chr(10), ' ')}...")
    else:
        print("   âš ï¸ Warning: No test code found in Markdown content!")

    # 5. ç»„è£…/æ‰å¹³åŒ– (Flatten)
    # å°†æå–åˆ°çš„ä»£ç å’Œæ¨ç†è¿‡ç¨‹åˆå¹¶è¿› meta å­—å…¸ï¼Œæ–¹ä¾¿åç»­èŠ‚ç‚¹ä½¿ç”¨
    meta["content"] = test_code
    meta["reasoning"] = reasoning # æŠŠ PBT çš„æ¨ç†æ€è·¯ä¹Ÿå­˜ä¸‹æ¥ï¼ŒDebug æ—¶å¾ˆæœ‰ç”¨
    
    # å…œåº•ï¼šå¦‚æœ LLM å¿˜äº†å†™ filenameï¼Œé»˜è®¤ç»™ä¸€ä¸ª
    if "filename" not in meta or not meta["filename"]:
        meta["filename"] = "test.py"

    return meta




def cleaner_debug_agent(llm_output: str) -> dict:
    """Logic from 'Debug Cleaner' node."""

    print("=============================================")
    llm_output = llm_output.strip("\n`markdown") + "\n```"
    # print(llm_output)

    print("=============================================")
    


    # 1. å®šä¹‰ Schema (ä¸¥æ ¼å¯¹åº” PROMPT_DEBUG_SYSTEM ä¸­çš„ Markdown æ ‡é¢˜)
    debug_schema = {
        "Reasoning":    ("reasoning", "text"),      # å¯¹åº” ## Reasoning
        "Target_file":  ("target_file", "text"),    # å¯¹åº” ## Target_file (SOURCE/TEST)
        "File_content": ("file_content", "code")    # å¯¹åº” ## File_content (æå–ä»£ç å—)
    }

    print(f"ğŸ§¹ [Cleaner] Parsing Debug Output...")
    
    # 2. è°ƒç”¨é€šç”¨è§£æå™¨
    cleaned_data = parse_markdown_with_schema(llm_output, debug_schema)
    
    # 3. æå–å¹¶æ¸…æ´—æ•°æ®
    reasoning = cleaned_data.get("reasoning", "")


    
    # æ¸…æ´— Target_file: ç§»é™¤å¯èƒ½å­˜åœ¨çš„ç©ºæ ¼ã€æ¢è¡Œæˆ– Markdown åŠ ç²—ç¬¦å· (**SOURCE**)
    raw_target = cleaned_data.get("target_file", "").strip().upper().replace("*", "")
    if "SOURCE" in raw_target:
        target_file = "SOURCE"
    elif "TEST" in raw_target:
        target_file = "TEST"
    # target_file = raw_target.strip().upper().replace("*", "") if raw_target else "SOURCE"
    
    # æå–ä»£ç å†…å®¹
    file_content = cleaned_data.get("file_content")

    if not reasoning or not raw_target or not target_file:
        with open("logs.txt", "a") as f:
            f.write("================================================\n" * 2+llm_output)

    # 4. æ‰“å°è°ƒè¯•ä¿¡æ¯ (Log)
    print(f"   ğŸ¯ Decision: Fix {target_file}")
    print(f"   ğŸ§  Reasoning: {reasoning.strip().splitlines()[0]}..." if reasoning else "   No reasoning found")
    
    if file_content:
        print(f"   ğŸ“ Code Preview: {file_content[:60].replace(chr(10), ' ')}...")
    else:
        print("   âš ï¸ Warning: No code content found in Debug output!")

    # 5. è¿”å›æ ‡å‡†åŒ–å­—å…¸
    return {
        "target_file": target_file,
        "file_content": file_content,
        "reasoning": reasoning
    }




def cleaner_debug_test(llm_output: str) -> dict:
    """Logic from 'Debug Cleaner' node."""

    print("=============================================")
    llm_output = llm_output.strip("\n`markdown") + "\n```"
    # print(llm_output)

    print("=============================================")
    


    # 1. å®šä¹‰ Schema (ä¸¥æ ¼å¯¹åº” PROMPT_DEBUG_SYSTEM ä¸­çš„ Markdown æ ‡é¢˜)
    debug_schema = {
        "Reasoning":    ("reasoning", "text"),      # å¯¹åº” ## Reasoning
        "Decision":  ("decision", "text"),    # å¯¹åº” ## Decision
        "File_content": ("file_content", "code")    # å¯¹åº” ## File_content (æå–ä»£ç å—)
    }

    print(f"ğŸ§¹ [Cleaner] Parsing Debug [TESTCASE] Output...")
    
    # 2. è°ƒç”¨é€šç”¨è§£æå™¨
    cleaned_data = parse_markdown_with_schema(llm_output, debug_schema)
    
    # 3. æå–å¹¶æ¸…æ´—æ•°æ®
    reasoning = cleaned_data.get("reasoning", "")


    
    # æ¸…æ´— Target_file: ç§»é™¤å¯èƒ½å­˜åœ¨çš„ç©ºæ ¼ã€æ¢è¡Œæˆ– Markdown åŠ ç²—ç¬¦å· (**SOURCE**)
    decision = cleaned_data.get("decision", "").strip().upper().replace("*", "")
    if "FIX" in decision:
        decision = "FIX"
    elif "REMAIN" in decision:
        decision = "REMAIN"
    # target_file = raw_target.strip().upper().replace("*", "") if raw_target else "SOURCE"
    
    # æå–ä»£ç å†…å®¹
    file_content = cleaned_data.get("file_content")

    if not reasoning or not decision or not file_content:
        with open("logs.txt", "a") as f:
            f.write("================================================\n" * 2+llm_output)

    # 4. æ‰“å°è°ƒè¯•ä¿¡æ¯ (Log)
    print(f"   ğŸ¯ Decision: Fix {decision}")
    print(f"   ğŸ§  Reasoning: {reasoning.strip().splitlines()[0]}..." if reasoning else "   No reasoning found")
    
    if file_content:
        print(f"   ğŸ“ Code Preview: {file_content[:60].replace(chr(10), ' ')}...")
    else:
        print("   âš ï¸ Warning: No code content found in TESTCASE output!")

    # 5. è¿”å›æ ‡å‡†åŒ–å­—å…¸
    return {
        "decision": decision,
        "file_content": file_content,
        "reasoning": reasoning
    }




def cleaner_debug_source(llm_output: str) -> dict:
    """Logic from 'Debug Cleaner' node."""

    print("=============================================")
    llm_output = llm_output.strip("\n`markdown") + "\n```"
    # print(llm_output)

    print("=============================================")
    


    # 1. å®šä¹‰ Schema (ä¸¥æ ¼å¯¹åº” PROMPT_DEBUG_SYSTEM ä¸­çš„ Markdown æ ‡é¢˜)
    debug_schema = {
        "Reasoning":    ("reasoning", "text"),      # å¯¹åº” ## Reasoning
        "Decision":  ("decision", "text"),    # å¯¹åº” ## Decision
        "File_content": ("file_content", "code")    # å¯¹åº” ## File_content (æå–ä»£ç å—)
    }

    print(f"ğŸ§¹ [Cleaner] Parsing Debug [SOURCE] Output...")
    
    # 2. è°ƒç”¨é€šç”¨è§£æå™¨
    cleaned_data = parse_markdown_with_schema(llm_output, debug_schema)
    
    # 3. æå–å¹¶æ¸…æ´—æ•°æ®
    reasoning = cleaned_data.get("reasoning", "")


    
    # æ¸…æ´— Target_file: ç§»é™¤å¯èƒ½å­˜åœ¨çš„ç©ºæ ¼ã€æ¢è¡Œæˆ– Markdown åŠ ç²—ç¬¦å· (**SOURCE**)
    decision = cleaned_data.get("decision", "").strip().upper().replace("*", "")
    if "FIX" in decision:
        decision = "FIX"
    elif "VETO" in decision:
        decision = "VETO"
    # target_file = raw_target.strip().upper().replace("*", "") if raw_target else "SOURCE"
    
    # æå–ä»£ç å†…å®¹
    file_content = cleaned_data.get("file_content")

    if not reasoning or not decision or not file_content:
        with open("logs.txt", "a") as f:
            f.write("================================================\n" * 2+llm_output)

    # 4. æ‰“å°è°ƒè¯•ä¿¡æ¯ (Log)
    print(f"   ğŸ¯ Decision: Fix {decision}")
    print(f"   ğŸ§  Reasoning: {reasoning.strip().splitlines()[0]}..." if reasoning else "   No reasoning found")
    
    if file_content:
        print(f"   ğŸ“ Code Preview: {file_content[:60].replace(chr(10), ' ')}...")
    else:
        print("   âš ï¸ Warning: No code content found in SOURCE output!")

    # 5. è¿”å›æ ‡å‡†åŒ–å­—å…¸
    return {
        "decision": decision,
        "file_content": file_content,
        "reasoning": reasoning
    }






# ============================================================================
# Tool: Code Execution (Subprocess)
# ============================================================================

def execute_code(source_code: dict, test_code: dict, run_command=None) -> Dict[str, Any]:
    """
    Simulates the Vercel Code Runner.
    Writes code to temp files and runs unittest.
    """

    with Sandbox.create(api_key=E2B_API_KEY) as sandbox:
        print("ğŸš€ æ²™ç®±å·²å¯åŠ¨...")

        packages = source_code["packages"]

        if packages:
            package_str = " ".join(packages)
            print(f"installing {package_str}")
            install_cmd = f"pip install {package_str}"
            sandbox.commands.run(install_cmd, timeout=120)

        filename = source_code["filename"]
        sandbox.files.write(filename, source_code["content"])
        print(f"âœ… æ–‡ä»¶ {filename} å·²å†™å…¥æ²™ç®±")

        test_filename = test_code["filename"]
        test_file_content = f"import unittest\nfrom {filename.split('.')[0]} import *\n\n" + test_code["content"]
        sandbox.files.write(test_filename, test_file_content)
        print(f"âœ… æ–‡ä»¶ {test_filename} å·²å†™å…¥æ²™ç®±")


        # 3. æ‰§è¡Œä»£ç  (æ›¿ä»£ subprocess çš„éƒ¨åˆ†)
        # æ³¨æ„ï¼štimeout å‚æ•°ç›´æ¥åœ¨è¿™é‡Œè®¾ç½®ï¼Œå•ä½æ˜¯ç§’
        if run_command is None:
            cmd = f"python -m unittest {test_filename}"
        else:
            cmd = run_command
        try:
            proc = sandbox.commands.run(cmd, timeout=30)
            
            # E2B çš„ proc å¯¹è±¡ç›´æ¥æä¾›äº† exit_code, stdout, stderr
            is_pass = proc.exit_code == 0
            
            return {
                "is_pass": is_pass,
                "stderr": proc.stderr,
                "stdout": proc.stdout,
                # å¦‚æœæµ‹è¯•é€šè¿‡ï¼Œerror ä¸ºç©ºï¼›å¦åˆ™æç¤º Failed
                "error": "" if is_pass else "Tests failed"
            }

        except TimeoutError:
            # E2B å†…éƒ¨è¶…æ—¶ä¼šæŠ›å‡º TimeoutError
            return {
                "is_pass": False,
                "stderr": "Execution Timed Out (30s)",
                "stdout": "",
                "error": "Timeout"
            }
        except Exception as e:
            # æ•è·æ‰§è¡Œè¿‡ç¨‹ä¸­çš„å…¶ä»–é”™è¯¯ï¼ˆå¦‚æ²™ç®±å†…éƒ¨å´©æºƒç­‰ï¼‰
            return {
                "is_pass": False,
                "stderr": str(e),
                "stdout": "",
                "error": "Execution Error during run"
            }


def qa_judge(exec_result: Dict[str, Any]) -> Tuple[bool, str]:
    """Logic from 'QA Judge' node."""
    is_pass = exec_result.get("is_pass", False)
    error_log = ""
    
    if not is_pass:
        error_log = f"Summary: Tests Failed\n"
        error_log += f"Details:\n{exec_result.get('stderr')}\n"
        if exec_result.get('error'):
             error_log += f"Traceback:\n{exec_result.get('error')}"
             
    return is_pass, error_log

# ============================================================================
# Prompts (Hardcoded from YAML)
# ============================================================================

from prompts import (
    PROMPT_PM_SYSTEM,
    PROMPT_CODER_SYSTEM,
    PROMPT_TESTCASE_SYSTEM,
    # PROMPT_DEBUG_SYSTEM,
    PROMPT_DEBUG_TESTCASE,
    PROMPT_DEBUG_SOURCE,
)

# ============================================================================
# Main Agent Flow
# ============================================================================

def main(prompt=None,max_loop_count=3, return_code=False, return_stats=True):
    if len(sys.argv) < 2:
        if prompt is None:
            print("Usage: python coding_agent.py \"Your project requirement\"")
            sys.exit(1)
        else: user_requirement = prompt
    else:
        user_requirement = sys.argv[1]
    
    current_task_stats = {
        "tokens": 0,
        "loops": 0,
        "calls": 0,
    }

    print(f"ğŸš€ CodingAgent Started")
    print(f"ğŸ“‹ Requirement: {user_requirement}\n")

    # 1. PM Agent
    print("ğŸ¤– [PM Agent] Analyzing requirements...")
    pm_response = call_llm(PROMPT_PM_SYSTEM, user_requirement, current_task_stats)
    with open("pm.txt", "w", encoding="utf-8") as f:
        f.write(pm_response)
    user_story = pm_response
    print("âœ… User Story Generated.\n")
    
    # 2. Initial Coder
    print("ğŸ‘¨â€ğŸ’» [Initial Coder] Writing code...")
    coder_response = call_llm(PROMPT_CODER_SYSTEM, user_story, current_task_stats)
    with open("coder.txt", "w", encoding="utf-8") as f:
        f.write(coder_response)

    current_code = cleaner_source_code(coder_response)
    print("âœ… Initial Code Generated.\n")

    # 3. Testcase Agent
    print("ğŸ§ª [Testcase Agent] Generating tests...")
    test_user_prompt = f"**User Story**: {user_story}\n**Source Code**: {current_code['content']}"
    test_response = call_llm(PROMPT_TESTCASE_SYSTEM, test_user_prompt, current_task_stats)
    with open("test.txt", "w", encoding="utf-8") as f:
        f.write(test_response)

    current_testcase = cleaner_test_case(test_response)
    print("âœ… Test Cases Generated.\n")
    
    # Loop Configuration
    max_loops = max_loop_count
    loop_count = 0
    final_success = False

    while loop_count < max_loops:
        loop_count += 1
        print(f"ğŸ”„ [Loop {loop_count}/{max_loops}] Executing & Testing...")
        
        # 4. Execute Code
        exec_result = execute_code(current_code, current_testcase)
        is_pass, error_log = qa_judge(exec_result)
        


        if is_pass:
            print("ğŸ‰ [QA Judge] Tests Passed!")
            print(exec_result)
            final_success = True # æ ‡è®°æˆåŠŸ
            break
        # else:
        print("âŒ [QA Judge] Tests Failed.")
        print(exec_result)
        print(f"   Error Summary: {error_log.splitlines()[0] if error_log else 'Unknown'}")
        
        if loop_count == max_loops:
            print("âš ï¸ Max loops reached. Exiting with last version.")
            break
        
        # 5. Debug Agent
        print("ğŸ”§ [Debug Agent] Analyzing failure in TESTCASE...")
        debug_prompt = (
            f"**Source Code**: {current_code['content']}\n\n"
            f"**Test Case**: {current_testcase['content']}\n\n"
            f"**User Story**: {user_story}\n\n"
            f"**Execution Output**: {error_log}\n"
        )
        debug_response = call_llm(PROMPT_DEBUG_TESTCASE, debug_prompt, current_task_stats)
        with open(f"debug_test_{loop_count}.txt", "w", encoding="utf-8") as f:
            f.write(debug_response)

        debug_fix = cleaner_debug_test(debug_response)
        
        decision = debug_fix.get("decision")
        content = debug_fix.get("file_content")
        
        if decision == "FIX":
            print("ğŸ› ï¸  Fixing Testcase Code...")
            current_testcase["content"] = content #, "packages": current_code["packages"]}
            re_execute = True
        elif decision == "REMAIN":
            print("Test Case has no error, proceed to next step...")
            re_execute = False
            # current_testcase["content"] = content
        else:
            print("âš ï¸ Debug Agent returned unknown target. Stopping.")
            break
        
        if re_execute:
            exec_result = execute_code(current_code, current_testcase)
            is_pass, error_log = qa_judge(exec_result)
        
            if is_pass:
                print("ğŸ‰ [QA Judge] Tests Passed!")
                print(exec_result)
                final_success = True # æ ‡è®°æˆåŠŸ
                break
            # else:
            print("âŒ [QA Judge] Tests Failed.")
            print(exec_result)
            print(f"   Error Summary: {error_log.splitlines()[0] if error_log else 'Unknown'}")


        # 6. Debug Agent
        print("ğŸ”§ [Debug Agent] Analyzing failure in SOURCE...")
        debug_prompt = (
            f"**Source Code**: {current_code['content']}\n\n"
            f"**Test Case**: {current_testcase['content']}\n\n"
            f"**User Story**: {user_story}\n\n"
            f"**Execution Output**: {error_log}\n"
        )
        debug_response = call_llm(PROMPT_DEBUG_SOURCE, debug_prompt, current_task_stats)
        with open(f"debug_source_{loop_count}.txt", "w", encoding="utf-8") as f:
            f.write(debug_response)

        debug_fix = cleaner_debug_source(debug_response)
        
        decision = debug_fix.get("decision")
        content = debug_fix.get("file_content")
        
        if decision == "FIX":
            print("ğŸ› ï¸  Fixing Source Code...")
            current_code["content"] = content #, "packages": current_code["packages"]}
            # re_execute = True
        elif decision == "VETO":
            print("Source Code has no error, proceed to next step...")
            # re_execute = False
            # current_testcase["content"] = content
        else:
            print("âš ï¸ Debug Agent returned unknown target. Stopping.")
            break



    # End
    output_dir = "output"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    filename = current_code["filename"]
    test_filename = current_testcase["filename"]

    final_main_path = os.path.join(output_dir, filename)
    final_test_path = os.path.join(output_dir, test_filename)

    if current_code['packages']:
        print("Writing requirements.txt")
        final_requirement_path = os.path.join(output_dir, "requirements.txt")
        with open(final_requirement_path, "w", encoding="utf-8") as f:
            f.write("\n".join(current_code['packages']))


    with open(final_main_path, "w", encoding="utf-8") as f:
        f.write(current_code["content"])
        
    with open(final_test_path, "w", encoding="utf-8") as f:
        f.write(f"import unittest\nfrom {filename.split('.')[0]} import *\n\n{current_testcase['content']}")
        
    print("\nâœ¨ Process Completed!")
    print(f"ğŸ“‚ Final Code: {final_main_path}")
    print(f"ğŸ“‚ Final Test: {final_test_path}")

    current_task_stats["loops"] = loop_count + (not final_success)
    result_code_dict = {
                "is_pass": final_success,
                "code": current_code,
                "test": current_testcase,
                # "task_id": getattr(main, "current_task_id", "unknown") # å¯é€‰ï¼šè®°å½•ä»»åŠ¡ID
            }
    if return_code:
        if return_stats:
            current_task_stats.update(result_code_dict)
            return current_task_stats
        else:
            return result_code_dict
    else:
        if return_stats:
            return current_task_stats

if __name__ == "__main__":
    main()
